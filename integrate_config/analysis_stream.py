# # from pyspark.sql import SparkSession
# # from pyspark.sql.functions import from_json, col
# # from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType
# #
# # # Kafka ë©”ì‹œì§€ì˜ JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
# # schema = StructType([
# #     StructField("analysis_type", StringType(), True),
# #     StructField("timestamp", TimestampType(), True),
# #     StructField("data", ArrayType(
# #         StructType([
# #             StructField("id", StringType(), True),
# #             StructField("text", StringType(), True),
# #         ])
# #     ), True),
# # ])
# #
# # # Spark ì„¸ì…˜ ì‹œì‘
# # spark = SparkSession.builder \
# #     .appName("KafkaAnalysisStream") \
# #     .getOrCreate()
# #
# # # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
# # spark.sparkContext.setLogLevel("WARN")
# #
# # # Kafka ì—°ê²° ë¡œê·¸ ì¶œë ¥
# # print("ğŸ“¡ Connecting to Kafka at kafka-container:9092 on topic ANALYSIS_REQUEST_TOPIC")
# # print(f"ğŸ“‘ Using schema: {schema.simpleString()}")
# #
# # # Kafkaë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹ 
# # df = spark.readStream.format("kafka") \
# #     .option("kafka.bootstrap.servers", "kafka-container:9092") \
# #     .option("subscribe", "ANALYSIS_REQUEST_TOPIC") \
# #     .option("startingOffsets", "latest") \
# #     .load()
# #
# # # Kafka valueëŠ” ë°”ì´ë„ˆë¦¬ -> ë¬¸ìì—´ -> JSON íŒŒì‹±
# # parsed = df.selectExpr("CAST(value AS STRING) AS value_str") \
# #     .withColumn("json", from_json(col("value_str"), schema)) \
# #     .withColumn("parse_failed", col("json").isNull()) \
# #     .select("value_str", "json.*", "parse_failed")
# #
# # # ì¶œë ¥ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë¡œì§
# # def process_batch(batch_df, batch_id):
# #     print(f"\nğŸ“¦ New Batch Received! ID: {batch_id} | Total records: {batch_df.count()}")
# #
# #     try:
# #         # JSON íŒŒì‹± ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ì¶œë ¥
# #         failed = batch_df.filter(col("parse_failed") == True)
# #         if failed.count() > 0:
# #             print("âŒ Failed to parse the following messages:")
# #             failed.select("value_str").show(truncate=False)
# #
# #         # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ë©”ì‹œì§€ ì¶œë ¥
# #         valid = batch_df.filter(col("parse_failed") == False).drop("parse_failed", "value_str")
# #         if valid.count() > 0:
# #             print("âœ… Successfully parsed messages:")
# #             valid.show(truncate=False)
# #
# #     except Exception as e:
# #         print(f"ğŸ”¥ Error while processing batch {batch_id}: {e}")
# #
# # # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹¤í–‰
# # query = parsed.writeStream \
# #     .outputMode("append") \
# #     .foreachBatch(process_batch) \
# #     .option("checkpointLocation", "/tmp/spark_kafka_checkpoint") \
# #     .start()
# #
# # query.awaitTermination()
# #
# # # docker exec -it spark-master /opt/bitnami/spark/bin/spark-submit `
# # # >>   --master spark://spark-master:7077 `
# # # >>   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.2 `
# # # >>   /opt/bitnami/spark/jobs/analysis_stream.py
#
# from pyspark.sql import SparkSession
# from pyspark.sql.functions import from_json, col
# from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType
#
# # Kafka ë©”ì‹œì§€ì˜ JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
# schema = StructType([
#     StructField("analysis_type", StringType(), True),
#     StructField("timestamp", TimestampType(), True),
#     StructField("data", ArrayType(
#         StructType([
#             StructField("id", StringType(), True),
#             StructField("text", StringType(), True),
#         ])
#     ), True),
# ])
#
# # Spark ì„¸ì…˜ ì‹œì‘ (ë³´ì•ˆ ë° ì¸ì¦ ì„¤ì • ì¶”ê°€)
# spark = SparkSession.builder \
#     .appName("KafkaAnalysisStream") \
#     .config("spark.hadoop.fs.defaultFS", "file:///") \
#     .config("spark.hadoop.hadoop.security.authentication", "simple") \
#     .config("spark.hadoop.hadoop.security.authorization", "false") \
#     .config("spark.sql.streaming.checkpointLocation", "/tmp/spark_kafka_checkpoint") \
#     .getOrCreate()
#
# # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
# spark.sparkContext.setLogLevel("WARN")
#
# # Kafka ì—°ê²° ë¡œê·¸ ì¶œë ¥
# print("ğŸ“¡ Connecting to Kafka at kafka-container:9092 on topic ANALYSIS_REQUEST_TOPIC")
# print(f"ğŸ“‘ Using schema: {schema.simpleString()}")
#
# # Kafkaë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹ 
# df = spark.readStream.format("kafka") \
#     .option("kafka.bootstrap.servers", "kafka-container:9092") \
#     .option("subscribe", "ANALYSIS_REQUEST_TOPIC") \
#     .option("startingOffsets", "latest") \
#     .option("failOnDataLoss", "false") \
#     .load()
#
# # Kafka valueëŠ” ë°”ì´ë„ˆë¦¬ -> ë¬¸ìì—´ -> JSON íŒŒì‹±
# parsed = df.selectExpr("CAST(value AS STRING) AS value_str") \
#     .withColumn("json", from_json(col("value_str"), schema)) \
#     .withColumn("parse_failed", col("json").isNull()) \
#     .select("value_str", "json.*", "parse_failed")
#
# # ì¶œë ¥ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë¡œì§
# def process_batch(batch_df, batch_id):
#     print(f"\nğŸ“¦ New Batch Received! ID: {batch_id} | Total records: {batch_df.count()}")
#
#     try:
#         # JSON íŒŒì‹± ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ì¶œë ¥
#         failed = batch_df.filter(col("parse_failed") == True)
#         if failed.count() > 0:
#             print("âŒ Failed to parse the following messages:")
#             failed.select("value_str").show(truncate=False)
#
#         # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ë©”ì‹œì§€ ì¶œë ¥
#         valid = batch_df.filter(col("parse_failed") == False).drop("parse_failed", "value_str")
#         if valid.count() > 0:
#             print("âœ… Successfully parsed messages:")
#             valid.show(truncate=False)
#
#     except Exception as e:
#         print(f"ğŸ”¥ Error while processing batch {batch_id}: {e}")
#
# # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹¤í–‰
# query = parsed.writeStream \
#     .outputMode("append") \
#     .foreachBatch(process_batch) \
#     .start()
#
# print("â³ Waiting for streaming data...")
# query.awaitTermination()

from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType

# Kafka ë©”ì‹œì§€ì˜ JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = StructType([
    StructField("analysis_type", StringType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("data", ArrayType(
        StructType([
            StructField("id", StringType(), True),
            StructField("text", StringType(), True),
        ])
    ), True),
])

# Spark ì„¸ì…˜ ì‹œì‘ (ë³´ì•ˆ ë° ì¸ì¦ ì„¤ì • ì¶”ê°€)
# spark = SparkSession.builder \
#     .appName("KafkaAnalysisStream") \
#     .config("spark.hadoop.fs.defaultFS", "file:///") \
#     .config("spark.hadoop.hadoop.security.authentication", "simple") \
#     .config("spark.hadoop.hadoop.security.authorization", "false") \
#     .config("spark.sql.streaming.checkpointLocation", "/tmp/spark_kafka_checkpoint") \
#     .getOrCreate()

spark = SparkSession.builder \
    .appName("KafkaAnalysisStream") \
    .config("spark.hadoop.fs.defaultFS", "file:///") \
    .config("spark.hadoop.fs.AbstractFileSystem.viewfs.impl", "org.apache.hadoop.fs.UnsupportedFileSystem") \
    .config("spark.hadoop.fs.viewfs.impl.disable.cache", "true") \
    .config("spark.hadoop.hadoop.security.authentication", "simple") \
    .config("hadoop.security.authentication", "simple") \
    .config("spark.hadoop.hadoop.security.authorization", "false") \
    .config("spark.yarn.principal", "") \
    .config("spark.yarn.keytab", "") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/spark_kafka_checkpoint") \
    .getOrCreate()

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
spark.sparkContext.setLogLevel("WARN")

# Kafka ì—°ê²° ë¡œê·¸ ì¶œë ¥
print("ğŸ“¡ Connecting to Kafka at kafka:9092 on topic ANALYSIS_REQUEST_TOPIC")
print(f"ğŸ“‘ Using schema: {schema.simpleString()}")

# Kafkaë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹ 
# ë°ì´í„° ì†ì‹¤ ì‹œ ì‹¤íŒ¨í•˜ì§€ ì•Šë„ë¡ ì„¤ì •
df = spark.readStream.format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "ANALYSIS_REQUEST_TOPIC") \
    .option("startingOffsets", "latest") \
    .option("failOnDataLoss", "false") \
    .load()

# Kafka valueëŠ” ë°”ì´ë„ˆë¦¬ -> ë¬¸ìì—´ -> JSON íŒŒì‹±
parsed = df.selectExpr("CAST(value AS STRING) AS value_str") \
    .withColumn("json", from_json(col("value_str"), schema)) \
    .withColumn("parse_failed", col("json").isNull()) \
    .select("value_str", "json.*", "parse_failed")

# ì¶œë ¥ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë¡œì§
def process_batch(batch_df, batch_id):
    print(f"\nğŸ“¦ New Batch Received! ID: {batch_id} | Total records: {batch_df.count()}")

    try:
        # JSON íŒŒì‹± ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ì¶œë ¥
        failed = batch_df.filter(col("parse_failed") == True)
        if failed.count() > 0:
            print("âŒ Failed to parse the following messages:")
            failed.select("value_str").show(truncate=False)

        # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ë©”ì‹œì§€ ì¶œë ¥
        valid = batch_df.filter(col("parse_failed") == False).drop("parse_failed", "value_str")
        if valid.count() > 0:
            print("âœ… Successfully parsed messages:")
            valid.show(truncate=False)

    except Exception as e:
        print(f"ğŸ”¥ Error while processing batch {batch_id}: {e}")

spark.sparkContext.setLogLevel("DEBUG")

# ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹¤í–‰
query = parsed.writeStream \
    .outputMode("append") \
    .foreachBatch(process_batch) \
    .start()

print("â³ Waiting for streaming data...")
query.awaitTermination()