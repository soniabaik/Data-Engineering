from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType

# Kafka ë©”ì‹œì§€ì˜ JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = StructType([
    StructField("analysis_type", StringType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("data", ArrayType(
        StructType([
            StructField("id", StringType(), True),
            StructField("text", StringType(), True),
        ])
    ), True),
])

# Spark ì„¸ì…˜ ì‹œìž‘
spark = SparkSession.builder \
    .appName("KafkaAnalysisStream") \
    .getOrCreate()

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
spark.sparkContext.setLogLevel("WARN")

# Kafka ì—°ê²° ë¡œê·¸ ì¶œë ¥
print("ðŸ“¡ Connecting to Kafka at localhost:9092 on topic ANALYSIS_REQUEST_TOPIC")
print(f"ðŸ“‘ Using schema: {schema.simpleString()}")

# Kafkaë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹ 
df = spark.readStream.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "ANALYSIS_REQUEST_TOPIC") \
    .option("startingOffsets", "latest") \
    .load()

# Kafka valueëŠ” ë°”ì´ë„ˆë¦¬ -> ë¬¸ìžì—´ -> JSON íŒŒì‹±
parsed = df.selectExpr("CAST(value AS STRING) AS value_str") \
    .withColumn("json", from_json(col("value_str"), schema)) \
    .withColumn("parse_failed", col("json").isNull()) \
    .select("value_str", "json.*", "parse_failed")

# ì¶œë ¥ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë¡œì§
def process_batch(batch_df, batch_id):
    print(f"\nðŸ“¦ New Batch Received! ID: {batch_id} | Total records: {batch_df.count()}")

    try:
        # JSON íŒŒì‹± ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ì¶œë ¥
        failed = batch_df.filter(col("parse_failed") == True)
        if failed.count() > 0:
            print("âŒ Failed to parse the following messages:")
            failed.select("value_str").show(truncate=False)

        # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ë©”ì‹œì§€ ì¶œë ¥
        valid = batch_df.filter(col("parse_failed") == False).drop("parse_failed", "value_str")
        if valid.count() > 0:
            print("âœ… Successfully parsed messages:")
            valid.show(truncate=False)

    except Exception as e:
        print(f"ðŸ”¥ Error while processing batch {batch_id}: {e}")

# ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹¤í–‰
query = parsed.writeStream \
    .outputMode("append") \
    .foreachBatch(process_batch) \
    .option("checkpointLocation", "/tmp/spark_kafka_checkpoint") \
    .start()

query.awaitTermination()

# docker exec -it spark-master /opt/bitnami/spark/bin/spark-submit `
# >>   --master spark://spark-master:7077 `
# >>   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.2 `
# >>   /opt/bitnami/spark/jobs/analysis_stream.py