from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType
import socket

# ë¡œì»¬ IP ì„¤ì • (ë“œë¼ì´ë²„ í˜¸ìŠ¤íŠ¸ìš©)
local_ip = socket.gethostbyname(socket.gethostname())

# Kafka ë©”ì‹œì§€ì˜ JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = StructType([
    StructField("analysis_type", StringType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("data", ArrayType(
        StructType([
            StructField("id", StringType(), True),
            StructField("text", StringType(), True),
        ])
    ), True),
])

# Spark ì„¸ì…˜ ìƒì„± (ë¡œì»¬ì—ì„œë§Œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ì •)
spark = SparkSession.builder \
    .appName("KafkaAnalysisStream") \
    .master("local[*]") \
    .config("spark.driver.host", local_ip) \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/spark_kafka_checkpoint") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.3") \
    .getOrCreate()

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
spark.sparkContext.setLogLevel("WARN")

# Kafka ì—°ê²° ë¡œê·¸ ì¶œë ¥
print("ğŸ“¡ Connecting to Kafka at localhost:9092 on topic ANALYSIS_REQUEST_TOPIC")
print(f"ğŸ“‘ Using schema: {schema.simpleString()}")

# Kafka ë©”ì‹œì§€ ìˆ˜ì‹  (Kafka ì ‘ê·¼ì€ ë¬´ì¡°ê±´ localhostë¡œ ì œí•œ)
df = spark.readStream.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "marketing.analysis.request") \
    .option("startingOffsets", "latest") \
    .option("failOnDataLoss", "false") \
    .load()


# Kafka value â†’ ë¬¸ìì—´ â†’ JSON íŒŒì‹±
parsed = df.selectExpr("CAST(value AS STRING) AS value_str") \
    .withColumn("json", from_json(col("value_str"), schema)) \
    .withColumn("parse_failed", col("json").isNull()) \
    .select("value_str", "json.*", "parse_failed")

# ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§ ì •ì˜
def process_batch(batch_df, batch_id):
    print(f"\nğŸ“¦ New Batch Received! ID: {batch_id} | Total records: {batch_df.count()}")

    try:
        # íŒŒì‹±ì— ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ì²˜ë¦¬
        failed = batch_df.filter(col("parse_failed") == True)
        if failed.count() > 0:
            print("âŒ Failed to parse the following messages:")
            failed.select("value_str").show(truncate=False)

        # ì •ìƒì ìœ¼ë¡œ íŒŒì‹±ëœ ë©”ì‹œì§€ ì²˜ë¦¬
        valid = batch_df.filter(col("parse_failed") == False).drop("parse_failed", "value_str")
        if valid.count() > 0:
            print("âœ… Successfully parsed messages:")
            valid.show(truncate=False)

    except Exception as e:
        print(f"ğŸ”¥ Error while processing batch {batch_id}: {e}")

# ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹¤í–‰
query = parsed.writeStream \
    .outputMode("append") \
    .foreachBatch(process_batch) \
    .start()

print("â³ Waiting for streaming data...")
query.awaitTermination()
